{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinrus/progetto-visione/blob/main/src/neuralnetwork/classificator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFnLBfR6hTvB"
      },
      "source": [
        "### Librerie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HhXuldH3hTvI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "import matplotlib.style as style\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image as image_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "from keras import models, layers\n",
        "import tensorflow as tf \n",
        "\n",
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg # Plotting\n",
        "import tables #gestione di set di dati gerarchici\n",
        "\n",
        "#style.use('seaborn-whitegrid')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connessione a google drive"
      ],
      "metadata": {
        "id": "h26wPkGvi5cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9f3Blf84i9zg",
        "outputId": "19b6164a-9b9b-4ce1-b3c2-40624f99230f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVxwqG9ehTvK"
      },
      "source": [
        "### Constanti\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YmhLD6kThTvK"
      },
      "outputs": [],
      "source": [
        "pathDataset='/content/drive/MyDrive/Visione e Percezione/progetto-visione/dataset' #VINCENZO\n",
        "#pathDataset='../../assets/dataset/' LOCALE\n",
        "pathSaveModel='/content/drive/MyDrive/Visione e Percezione/progetto-visione/assets/models/'\n",
        "#pathSaveModel='../../assets/models/' LOCALE\n",
        "epoche=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RhBOktehTvL"
      },
      "source": [
        "### Check dataset\n",
        "\n",
        "1. Costruiamo un dizionario 'label' per memorizzare i nomi dei gesti che andremo ad indentificare\n",
        "2. Costruiamo un dizionario 'reverseLabel' per memorizzare a quale gesto è associato un determinato identificatore\n",
        "   // Label inserite all'interno del dataset nella prima cartella 00 che usero per i tutti i test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d3-zpAqShTvL",
        "outputId": "2841a48c-c145-4513-ea48-0bfd935ac200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label=\n",
            "{'06_index': 0, '10_down': 1, '08_palm_moved': 2, '05_thumb': 3, '01_palm': 4, '07_ok': 5, '04_fist_moved': 6, '02_l': 7, '03_fist': 8, '09_c': 9}\n",
            "ReverseLabel=\n",
            "{0: '06_index', 1: '10_down', 2: '08_palm_moved', 3: '05_thumb', 4: '01_palm', 5: '07_ok', 6: '04_fist_moved', 7: '02_l', 8: '03_fist', 9: '09_c'}\n"
          ]
        }
      ],
      "source": [
        "label = dict()\n",
        "reverseLabel = dict()\n",
        "count = 0\n",
        "for j in os.listdir(pathDataset + '/00'):\n",
        "    if not j.startswith('.'):\n",
        "        label[j] = count\n",
        "        reverseLabel[count] = j\n",
        "        count = count + 1\n",
        "\n",
        "print(\"Label=\")\n",
        "print(label)\n",
        "print(\"ReverseLabel=\")\n",
        "print(reverseLabel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1e6S7YghTvM"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "lettura dei dati memorizzandoli nell'array 'datas'\n",
        "memorizziamo il classificatore numerico per ogni immagine in 'datasNum'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb3dL1tehTvN"
      },
      "outputs": [],
      "source": [
        "\n",
        "datas = []\n",
        "datasNum = []\n",
        "\n",
        "countImage = 0  # count delle immagini che ci sono nel dataset\n",
        "\n",
        "for i in range(0, 10):  # ciclo su 10 cartelle del primo livello\n",
        "    for j in os.listdir(pathDataset+'0' + str(i) + '/'):\n",
        "         if not j.startswith('.'):  # evito le cartelle nascoste \n",
        "            count = 0  # conta le imagini di un gesto\n",
        "            for k in os.listdir(pathDataset+'0' + str(i) + '/' + j + '/'): # ciclo le immagini\n",
        "                img = Image.open(pathDataset + '0' + str(i) + '/' + j + '/' + k).convert('L') # leggo è converto in scala di grigi\n",
        "                img = img.resize((224, 224))\n",
        "                arr = np.array(img)\n",
        "                datas.append(arr) \n",
        "                count = count + 1\n",
        "\n",
        "            #     img = img.resize((224, 224)) ##img.resize((320, 120))#\n",
        "            #     datas.append(np.array(img))\n",
        "            #     count = count + 1\n",
        "            \n",
        "            datasNum.append(np.full((count, 1), label[j]))\n",
        "            countImage = countImage + count\n",
        "\n",
        "\n",
        "datasNum = np.array(datasNum)\n",
        "datasNum = datasNum.reshape(countImage, 1)\n",
        "\n",
        "print(\"[DEBUG] numero di immagini esaminate: \" + str(countImage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdjsLqN-hTvN"
      },
      "outputs": [],
      "source": [
        "#Visualizzazione delle immagini\n",
        "plt.figure(figsize=(20,20)) # specifying the overall grid size\n",
        "\n",
        "for i in range(0, 10):\n",
        "    # for x in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.title(reverseLabel[i])\n",
        "        plt.imshow(datas[i*200])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L24MZI-rhTvO"
      },
      "outputs": [],
      "source": [
        "#ridimensione del formato dell'immagini cosi da avere comprese tra 0 e 1\n",
        "datas = np.array(datas, dtype = 'float32')\n",
        "datas = datas.reshape(len(datas), 224, 224, 1)\n",
        "datas /= 255\n",
        "\n",
        "#conversione di datasNum in one-hot\n",
        "datasNum = np.array(datasNum)\n",
        "datasNum = datasNum.reshape(len(datas), 1)\n",
        "datasNum = to_categorical(datasNum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0hl0ijPhTvO"
      },
      "source": [
        "### Split dei dati tramite Skleanr \n",
        "divisione del dataset in 2 parti 80 - 20 (train) e 50 - 50 (validate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx4FBDDhhTvP"
      },
      "outputs": [],
      "source": [
        "xTrain, xTest, yTrain, yTest = train_test_split(datas, datasNum, test_size=0.2)\n",
        "xValidation, xTest, yValidation, yTest = train_test_split(datas, datasNum, test_size=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7YbWPpzhTvP"
      },
      "source": [
        "# Modello tramite keras: \n",
        "\n",
        "Inzio ad aggiungere al modello di keras un filtro 5*5 con passo 2. \n",
        "Aggiungiamo una sequenza di livelli di convoluzione seguiti da un raggruppamento massimo fino a un'immagine piccola\n",
        "In fine aggiungiamo uno strato di softmax con 10 neuroni\n",
        "\n",
        "Di seguito adattiamo il modello "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6I2QgUyhTvP"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu', input_shape=(224, 224,1))) \n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIQFSz3AhTvQ"
      },
      "source": [
        "Allenamentodellarete--con10epocheiniziali--"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf6JoODmhTvQ"
      },
      "outputs": [],
      "source": [
        "history = model.fit(xTrain, yTrain, \n",
        "        epochs=epoche,\n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_data=(xValidation, yValidation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1B76_2BhTvR"
      },
      "source": [
        "### Plot History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtEMuRG0hTvR"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epoche) \n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2vf7CybhTvS"
      },
      "outputs": [],
      "source": [
        "#Preparazione della 'Matrice di convoluzione'\n",
        "y_pred=model.predict(xTest) \n",
        "y_pred=np.argmax(yValidation, axis=1)\n",
        "y_test=np.argmax(yTest, axis=1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "# print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CvK0BEIhTvS"
      },
      "outputs": [],
      "source": [
        "#Plot Matrice di convoluzione \n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "plt.title(\"Confusion Matrix\")\n",
        "disp = disp.plot(ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef-FD2EwhTvT"
      },
      "outputs": [],
      "source": [
        "# valutazione delle previsioni della rete  \n",
        "valutazione = model.predict(xTest, batch_size=64) \n",
        "print(classification_report(yTest.argmax(axis=1),\n",
        "                            valutazione.argmax(axis=1), \n",
        "                            target_names=label))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJOcC4XthTvT"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcRqpdqehTvT"
      },
      "outputs": [],
      "source": [
        "#model.save(pathSaveModel) save all\n",
        "model.save(pathSaveModel+'model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8hAn1cShTvU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjwKUf4hTvU"
      },
      "source": [
        "# Creazione del modello con VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz7D2RtVhTvU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHkvvS5EhTvU"
      },
      "outputs": [],
      "source": [
        "# Get back the convolutional part of a VGG network trained on ImageNet\n",
        "model_vgg16_conv = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        ")\n",
        "model_vgg16_conv.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mSqr2PuhTvV"
      },
      "outputs": [],
      "source": [
        "# Crezionde dellimmaginie del formato (224x224x3) per li nostro input della rete\n",
        "imageInput = Input(shape=(224, 224, 3), name='imageInput')\n",
        "\n",
        "# makes the layers non-trainable\n",
        "for layer in model_vgg16_conv.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Uso il generatore di modello:\n",
        "outputVgg16Conv = model_vgg16_conv(imageInput)\n",
        "\n",
        "# Add the fully-connected layers \n",
        "x = Flatten(name='flatten')(outputVgg16Conv)\n",
        "x = Dense(10, activation='softmax', name='predictions')(x)  # here the 2 indicates binary (3 or more is multiclass)\n",
        "\n",
        "# Creazionde del nostro modello \n",
        "my_model = Model(imageInput, x)\n",
        "\n",
        "# In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
        "my_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IVNGKu1hTvV"
      },
      "outputs": [],
      "source": [
        "# ottimizzazione del modello: \n",
        "baseModel = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "optimizer = tf.keras.optimizers.Adam() # Adam is like a gradient descent (way to find parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofuNpg1DhTvV"
      },
      "outputs": [],
      "source": [
        "# Add top layer\n",
        "x = baseModel.output\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs=baseModel.input, outputs=predictions)\n",
        "\n",
        "# Train top layer\n",
        "for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "            optimizer=optimizer, \n",
        "            metrics=['accuracy'])\n",
        "\n",
        "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVv0Rk_XhTvW"
      },
      "outputs": [],
      "source": [
        "history = model.fit(xTrain[:500], \n",
        "        yTrain[:500], \n",
        "        epochs=4, \n",
        "        batch_size=64,\n",
        "        validation_data=(xValidation[:500], yValidation[:500]),\n",
        "        verbose=1\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kulR-mdhTvW"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epoche) \n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG0IQNMlhTvX"
      },
      "outputs": [],
      "source": [
        "#https://www.kaggle.com/code/benenharrington/hand-gesture-recognition-database-with-cnn/notebook\n",
        "#perdopo:https://techvidvan.com/tutorials/hand-gesture-recognition-tensorflow-opencv/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}